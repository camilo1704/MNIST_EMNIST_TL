{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMNIST con MNIST features\n",
    "\n",
    "En esta notebook se entrena una red, utilizando los features aprendidos de la base de datos del MNIST, para clasificar las clases de la base de datos de EMNIST, para ello se divide la red en la parte de feature layers de las capas convolucionales y la parte de clasificación de las capas densas. Se copian los pesos de las capas convolucionales aprendidos de MNIST y se entrenan los pesos de las capas de clasificación, obteniendo finalmente un accuracy de 93.68% para 15 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuchuflito/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import rotate\n",
    "import imageio\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from skimage import transform,io\n",
    "from resizeimage import resizeimage\n",
    "import scipy.io as sio\n",
    "import scipy.misc\n",
    "from skimage.transform import rescale, resize as rs, downscale_local_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('model_mnist.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impr(word):\n",
    "    print ('\\033[1m' + word + '\\033[0m')\n",
    "font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 15,\n",
    "        'size': 16,\n",
    "        }\n",
    "file=sio.loadmat('emnist-letters.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=file['dataset'][0,0][0][0][0][0]\n",
    "train_labels=file['dataset'][0,0][0][0][0][1]\n",
    "test=file['dataset'][0,0][1][0][0][0]\n",
    "test_labels=file['dataset'][0,0][1][0][0][1]\n",
    "labels_train=[]\n",
    "labels_test=[]\n",
    "for i in train_labels:\n",
    "    labels_train=np.append(labels_train,int(i[0]))\n",
    "for i in test_labels:\n",
    "    labels_test=np.append(labels_test,int(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.rot90(train[490].reshape(28,28)),cmap='gray')\n",
    "plt.show()\n",
    "from matplotlib.pyplot import *\n",
    "imsave('p.png',np.rot90(train[490].reshape(28,28)),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=labels_train-1   #empieza en 1 los labels----> les resto uno, sino hay problemas con el to_categorical \n",
    "test_labels=labels_test-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "train = train.reshape(train.shape[0], 28, 28, 1).astype('float32')/255\n",
    "test= test.reshape(test.shape[0], img_rows, img_cols, 1).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 26)                3354      \n",
      "=================================================================\n",
      "Total params: 1,201,946\n",
      "Trainable params: 1,183,130\n",
      "Non-trainable params: 18,816\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layers in model.layers:\n",
    "\tlayers.trainable=False\n",
    "model.layers[-1].trainable=True\n",
    "model.layers[-3].trainable=True\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "train_labels = keras.utils.to_categorical(train_labels,26)\n",
    "test_labels = keras.utils.to_categorical(test_labels,26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124800 samples, validate on 20800 samples\n",
      "Epoch 1/20\n",
      "124800/124800 [==============================] - 150s 1ms/step - loss: 1.7429 - acc: 0.4900 - val_loss: 0.5365 - val_acc: 0.8517\n",
      "Epoch 2/20\n",
      "124800/124800 [==============================] - 152s 1ms/step - loss: 0.8231 - acc: 0.7456 - val_loss: 0.3759 - val_acc: 0.8877\n",
      "Epoch 3/20\n",
      "124800/124800 [==============================] - 157s 1ms/step - loss: 0.6355 - acc: 0.8023 - val_loss: 0.3232 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "124800/124800 [==============================] - 146s 1ms/step - loss: 0.5553 - acc: 0.8266 - val_loss: 0.2947 - val_acc: 0.9074\n",
      "Epoch 5/20\n",
      "124800/124800 [==============================] - 149s 1ms/step - loss: 0.5032 - acc: 0.8426 - val_loss: 0.2779 - val_acc: 0.9121\n",
      "Epoch 6/20\n",
      "124800/124800 [==============================] - 146s 1ms/step - loss: 0.4670 - acc: 0.8534 - val_loss: 0.2691 - val_acc: 0.9172\n",
      "Epoch 7/20\n",
      "124800/124800 [==============================] - 150s 1ms/step - loss: 0.4474 - acc: 0.8589 - val_loss: 0.2665 - val_acc: 0.9155\n",
      "Epoch 8/20\n",
      "124800/124800 [==============================] - 151s 1ms/step - loss: 0.4231 - acc: 0.8667 - val_loss: 0.2580 - val_acc: 0.9200\n",
      "Epoch 9/20\n",
      "124800/124800 [==============================] - 147s 1ms/step - loss: 0.4087 - acc: 0.8700 - val_loss: 0.2523 - val_acc: 0.9219\n",
      "Epoch 10/20\n",
      "124800/124800 [==============================] - 141s 1ms/step - loss: 0.3941 - acc: 0.8745 - val_loss: 0.2512 - val_acc: 0.9228\n",
      "Epoch 11/20\n",
      "124800/124800 [==============================] - 142s 1ms/step - loss: 0.3813 - acc: 0.8782 - val_loss: 0.2439 - val_acc: 0.9237\n",
      "Epoch 12/20\n",
      "124800/124800 [==============================] - 145s 1ms/step - loss: 0.3718 - acc: 0.8817 - val_loss: 0.2454 - val_acc: 0.9232\n",
      "Epoch 13/20\n",
      "124800/124800 [==============================] - 147s 1ms/step - loss: 0.3568 - acc: 0.8854 - val_loss: 0.2376 - val_acc: 0.9258\n",
      "Epoch 14/20\n",
      "124800/124800 [==============================] - 142s 1ms/step - loss: 0.3488 - acc: 0.8884 - val_loss: 0.2369 - val_acc: 0.9257\n",
      "Epoch 15/20\n",
      "124800/124800 [==============================] - 139s 1ms/step - loss: 0.3421 - acc: 0.8890 - val_loss: 0.2390 - val_acc: 0.9255\n",
      "Epoch 16/20\n",
      "124800/124800 [==============================] - 140s 1ms/step - loss: 0.3377 - acc: 0.8909 - val_loss: 0.2355 - val_acc: 0.9263\n",
      "Epoch 17/20\n",
      "124800/124800 [==============================] - 138s 1ms/step - loss: 0.3287 - acc: 0.8930 - val_loss: 0.2341 - val_acc: 0.9280\n",
      "Epoch 18/20\n",
      "124800/124800 [==============================] - 138s 1ms/step - loss: 0.3258 - acc: 0.8932 - val_loss: 0.2321 - val_acc: 0.9279\n",
      "Epoch 19/20\n",
      "124800/124800 [==============================] - 143s 1ms/step - loss: 0.3172 - acc: 0.8970 - val_loss: 0.2306 - val_acc: 0.9276\n",
      "Epoch 20/20\n",
      "124800/124800 [==============================] - 145s 1ms/step - loss: 0.3148 - acc: 0.8974 - val_loss: 0.2304 - val_acc: 0.9274\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(train,train_labels,batch_size=128*4, epochs=20, verbose=1, validation_data=(test,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('EMNIST_with_MNIST_features.json', 'w') as f:\n",
    "    json.dump(hist.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 26)                3354      \n",
      "=================================================================\n",
      "Total params: 1,201,946\n",
      "Trainable params: 3,354\n",
      "Non-trainable params: 1,198,592\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layers in model.layers:\n",
    "\tlayers.trainable=False\n",
    "model.layers[-1].trainable=True\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124800 samples, validate on 20800 samples\n",
      "Epoch 1/20\n",
      "124800/124800 [==============================] - 133s 1ms/step - loss: 3.9209 - acc: 0.2677 - val_loss: 1.4629 - val_acc: 0.6112\n",
      "Epoch 2/20\n",
      "124800/124800 [==============================] - 134s 1ms/step - loss: 1.6521 - acc: 0.5330 - val_loss: 1.1911 - val_acc: 0.6965\n",
      "Epoch 3/20\n",
      "124800/124800 [==============================] - 134s 1ms/step - loss: 1.4988 - acc: 0.5725 - val_loss: 1.1193 - val_acc: 0.7137\n",
      "Epoch 4/20\n",
      "124800/124800 [==============================] - 139s 1ms/step - loss: 1.4548 - acc: 0.5850 - val_loss: 1.0848 - val_acc: 0.7174\n",
      "Epoch 5/20\n",
      "124800/124800 [==============================] - 141s 1ms/step - loss: 1.4316 - acc: 0.5891 - val_loss: 1.0628 - val_acc: 0.7224\n",
      "Epoch 6/20\n",
      "124800/124800 [==============================] - 134s 1ms/step - loss: 1.4179 - acc: 0.5933 - val_loss: 1.0501 - val_acc: 0.7246\n",
      "Epoch 7/20\n",
      "124800/124800 [==============================] - 133s 1ms/step - loss: 1.4059 - acc: 0.5962 - val_loss: 1.0395 - val_acc: 0.7265\n",
      "Epoch 8/20\n",
      "124800/124800 [==============================] - 132s 1ms/step - loss: 1.3994 - acc: 0.5967 - val_loss: 1.0315 - val_acc: 0.7253\n",
      "Epoch 9/20\n",
      "124800/124800 [==============================] - 133s 1ms/step - loss: 1.3939 - acc: 0.5960 - val_loss: 1.0255 - val_acc: 0.7271\n",
      "Epoch 10/20\n",
      "124800/124800 [==============================] - 138s 1ms/step - loss: 1.3948 - acc: 0.5959 - val_loss: 1.0232 - val_acc: 0.7256\n",
      "Epoch 11/20\n",
      "124800/124800 [==============================] - 135s 1ms/step - loss: 1.3856 - acc: 0.5979 - val_loss: 1.0174 - val_acc: 0.7266\n",
      "Epoch 12/20\n",
      "124800/124800 [==============================] - 133s 1ms/step - loss: 1.3822 - acc: 0.5999 - val_loss: 1.0128 - val_acc: 0.7242\n",
      "Epoch 13/20\n",
      "124800/124800 [==============================] - 132s 1ms/step - loss: 1.3856 - acc: 0.5980 - val_loss: 1.0106 - val_acc: 0.7257\n",
      "Epoch 14/20\n",
      "124800/124800 [==============================] - 132s 1ms/step - loss: 1.3853 - acc: 0.5975 - val_loss: 1.0095 - val_acc: 0.7247\n",
      "Epoch 15/20\n",
      "124800/124800 [==============================] - 124s 993us/step - loss: 1.3843 - acc: 0.5968 - val_loss: 1.0085 - val_acc: 0.7249\n",
      "Epoch 16/20\n",
      "124800/124800 [==============================] - 124s 997us/step - loss: 1.3771 - acc: 0.5991 - val_loss: 1.0053 - val_acc: 0.7273\n",
      "Epoch 17/20\n",
      "124800/124800 [==============================] - 126s 1ms/step - loss: 1.3777 - acc: 0.5976 - val_loss: 1.0053 - val_acc: 0.7251\n",
      "Epoch 18/20\n",
      "124800/124800 [==============================] - 125s 1ms/step - loss: 1.3804 - acc: 0.5976 - val_loss: 1.0046 - val_acc: 0.7270\n",
      "Epoch 19/20\n",
      "124800/124800 [==============================] - 132s 1ms/step - loss: 1.3748 - acc: 0.5972 - val_loss: 1.0019 - val_acc: 0.7274\n",
      "Epoch 20/20\n",
      "124800/124800 [==============================] - 142s 1ms/step - loss: 1.3758 - acc: 0.5971 - val_loss: 1.0020 - val_acc: 0.7246\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "train_labels = keras.utils.to_categorical(train_labels,26)\n",
    "test_labels = keras.utils.to_categorical(test_labels,26)\n",
    "hist=model.fit(train,train_labels,batch_size=128*4, epochs=20, verbose=1, validation_data=(test,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('EMNIST_with_MNIST_features_1.json', 'w') as f:\n",
    "    json.dump(hist.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1a1dcfdd436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
